{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the PyTorch model as it is in Tractolearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class AETorch(nn.Module):\n",
    "    \"\"\"Strided convolution-upsampling-based AE using reflection-padding and\n",
    "    increasing feature maps in decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_space_dims):\n",
    "        super(AETorch, self).__init__()\n",
    "\n",
    "        self.kernel_size = 3\n",
    "        self.latent_space_dims = latent_space_dims\n",
    "\n",
    "        self.pad = nn.ReflectionPad1d(1)\n",
    "\n",
    "        def pre_pad(m):\n",
    "            return nn.Sequential(self.pad, m)\n",
    "\n",
    "        self.encod_conv1 = pre_pad(\n",
    "            nn.Conv1d(3, 32, self.kernel_size, stride=2, padding=0)\n",
    "        )\n",
    "        self.encod_conv2 = pre_pad(\n",
    "            nn.Conv1d(32, 64, self.kernel_size, stride=2, padding=0)\n",
    "        )\n",
    "        self.encod_conv3 = pre_pad(\n",
    "            nn.Conv1d(64, 128, self.kernel_size, stride=2, padding=0)\n",
    "        )\n",
    "        self.encod_conv4 = pre_pad(\n",
    "            nn.Conv1d(128, 256, self.kernel_size, stride=2, padding=0)\n",
    "        )\n",
    "        self.encod_conv5 = pre_pad(\n",
    "            nn.Conv1d(256, 512, self.kernel_size, stride=2, padding=0)\n",
    "        )\n",
    "        self.encod_conv6 = pre_pad(\n",
    "            nn.Conv1d(512, 1024, self.kernel_size, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(8192, self.latent_space_dims)  # 8192 = 1024*8\n",
    "        self.fc2 = nn.Linear(self.latent_space_dims, 8192)\n",
    "\n",
    "        self.decod_conv1 = pre_pad(\n",
    "            nn.Conv1d(1024, 512, self.kernel_size, stride=1, padding=0)\n",
    "        )\n",
    "        self.upsampl1 = nn.Upsample(\n",
    "            scale_factor=2, mode=\"linear\", align_corners=False\n",
    "        )\n",
    "        self.decod_conv2 = pre_pad(\n",
    "            nn.Conv1d(512, 256, self.kernel_size, stride=1, padding=0)\n",
    "        )\n",
    "        self.upsampl2 = nn.Upsample(\n",
    "            scale_factor=2, mode=\"linear\", align_corners=False\n",
    "        )\n",
    "        self.decod_conv3 = pre_pad(\n",
    "            nn.Conv1d(256, 128, self.kernel_size, stride=1, padding=0)\n",
    "        )\n",
    "        self.upsampl3 = nn.Upsample(\n",
    "            scale_factor=2, mode=\"linear\", align_corners=False\n",
    "        )\n",
    "        self.decod_conv4 = pre_pad(\n",
    "            nn.Conv1d(128, 64, self.kernel_size, stride=1, padding=0)\n",
    "        )\n",
    "        self.upsampl4 = nn.Upsample(\n",
    "            scale_factor=2, mode=\"linear\", align_corners=False\n",
    "        )\n",
    "        self.decod_conv5 = pre_pad(\n",
    "            nn.Conv1d(64, 32, self.kernel_size, stride=1, padding=0)\n",
    "        )\n",
    "        self.upsampl5 = nn.Upsample(\n",
    "            scale_factor=2, mode=\"linear\", align_corners=False\n",
    "        )\n",
    "        self.decod_conv6 = pre_pad(\n",
    "            nn.Conv1d(32, 3, self.kernel_size, stride=1, padding=0)\n",
    "        )\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.encod_conv1(x))\n",
    "        h2 = F.relu(self.encod_conv2(h1))\n",
    "        h3 = F.relu(self.encod_conv3(h2))\n",
    "        h4 = F.relu(self.encod_conv4(h3))\n",
    "        h5 = F.relu(self.encod_conv5(h4))\n",
    "        h6 = self.encod_conv6(h5)\n",
    "\n",
    "        self.encoder_out_size = (h6.shape[1], h6.shape[2])\n",
    "\n",
    "        # Flatten\n",
    "        h7 = h6.view(-1, self.encoder_out_size[0] * self.encoder_out_size[1])\n",
    "\n",
    "        fc1 = self.fc1(h7)\n",
    "\n",
    "        return fc1\n",
    "\n",
    "    def decode(self, z):\n",
    "        fc = self.fc2(z)\n",
    "        fc_reshape = fc.view(\n",
    "            -1, self.encoder_out_size[0], self.encoder_out_size[1]\n",
    "        )\n",
    "        h1 = F.relu(self.decod_conv1(fc_reshape))\n",
    "        h2 = self.upsampl1(h1)\n",
    "        h3 = F.relu(self.decod_conv2(h2))\n",
    "        h4 = self.upsampl2(h3)\n",
    "        h5 = F.relu(self.decod_conv3(h4))\n",
    "        h6 = self.upsampl3(h5)\n",
    "        h7 = F.relu(self.decod_conv4(h6))\n",
    "        h8 = self.upsampl4(h7)\n",
    "        h9 = F.relu(self.decod_conv5(h8))\n",
    "        h10 = self.upsampl5(h9)\n",
    "        h11 = self.decod_conv6(h10)\n",
    "\n",
    "        return h11\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encode(x)\n",
    "        return self.decode(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 12:02:44.289059: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-18 12:02:44.370492: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-18 12:02:45.274225: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tractoencoder_gsoc.ae_model import IncrFeatStridedConvFCUpsampReflectPadAE as AEKeras\n",
    "model_torch = AETorch(32)\n",
    "model_keras = AEKeras(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the shape of the weights and biases in some layers between Torch and Keras to see the difference pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER LAYER CONV6\n",
      "Weights TORCH: torch.Size([1024, 512, 3])\n",
      "Biases TORCH: torch.Size([1024])\n",
      "---------------------------------\n",
      "Weights KERAS: (3, 512, 1024)\n",
      "Biases KERAS: (1024,)\n",
      "\n",
      "\n",
      "ENCODER LAYER CONV3\n",
      "Weights TORCH: torch.Size([128, 64, 3])\n",
      "Biases TORCH: torch.Size([128])\n",
      "---------------------------------\n",
      "Weights KERAS: (3, 64, 128)\n",
      "Biases KERAS: (128,)\n",
      "\n",
      "\n",
      "DECODER LAYER FC1\n",
      "Weights TORCH: torch.Size([32, 8192])\n",
      "Biases TORCH: torch.Size([32])\n",
      "---------------------------------\n",
      "Weights KERAS: (8192, 32)\n",
      "Biases KERAS: (32,)\n",
      "\n",
      "\n",
      "DECODER LAYER CONV1\n",
      "Weights TORCH: torch.Size([512, 1024, 3])\n",
      "Biases TORCH: torch.Size([512])\n",
      "---------------------------------\n",
      "Weights KERAS: (3, 1024, 512)\n",
      "Biases KERAS: (512,)\n"
     ]
    }
   ],
   "source": [
    "print(\"ENCODER LAYER CONV6\")\n",
    "print(f\"Weights TORCH: {model_torch.encod_conv6[1].weight.shape}\")\n",
    "print(f\"Biases TORCH: {model_torch.encod_conv6[1].bias.shape}\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Weights KERAS: {model_keras.model.get_layer('encoder').encod_conv6.layers[1].get_weights()[0].shape}\")\n",
    "print(f\"Biases KERAS: {model_keras.model.get_layer('encoder').encod_conv6.layers[1].get_weights()[1].shape}\")\n",
    "\n",
    "print(\"\\n\\nENCODER LAYER CONV3\")\n",
    "print(f\"Weights TORCH: {model_torch.encod_conv3[1].weight.shape}\")\n",
    "print(f\"Biases TORCH: {model_torch.encod_conv3[1].bias.shape}\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Weights KERAS: {model_keras.model.get_layer('encoder').encod_conv3.layers[1].get_weights()[0].shape}\")\n",
    "print(f\"Biases KERAS: {model_keras.model.get_layer('encoder').encod_conv3.layers[1].get_weights()[1].shape}\")\n",
    "\n",
    "print(\"\\n\\nDECODER LAYER FC1\")\n",
    "print(f\"Weights TORCH: {model_torch.fc1.weight.shape}\")\n",
    "print(f\"Biases TORCH: {model_torch.fc1.bias.shape}\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Weights KERAS: {model_keras.model.get_layer('encoder').fc1.get_weights()[0].shape}\")\n",
    "print(f\"Biases KERAS: {model_keras.model.get_layer('encoder').fc1.get_weights()[1].shape}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nDECODER LAYER CONV1\")\n",
    "print(f\"Weights TORCH: {model_torch.decod_conv1[1].weight.shape}\")\n",
    "print(f\"Biases TORCH: {model_torch.decod_conv1[1].bias.shape}\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Weights KERAS: {model_keras.model.get_layer('decoder').decod_conv1.layers[1].get_weights()[0].shape}\")\n",
    "print(f\"Biases KERAS: {model_keras.model.get_layer('decoder').decod_conv1.layers[1].get_weights()[1].shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ WEIGHTS FROM PTH FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "pth_path = os.path.abspath(\"/home/teitxe/data/tractolearn_data/best_model_contrastive_tractoinferno_hcp.pt\")\n",
    "torch_weights = torch.load(pth_path, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'state_dict', 'lowest_loss', 'optimizer'])\n"
     ]
    }
   ],
   "source": [
    "print(torch_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['encod_conv1.1.weight', 'encod_conv1.1.bias', 'encod_conv2.1.weight', 'encod_conv2.1.bias', 'encod_conv3.1.weight', 'encod_conv3.1.bias', 'encod_conv4.1.weight', 'encod_conv4.1.bias', 'encod_conv5.1.weight', 'encod_conv5.1.bias', 'encod_conv6.1.weight', 'encod_conv6.1.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'decod_conv1.1.weight', 'decod_conv1.1.bias', 'decod_conv2.1.weight', 'decod_conv2.1.bias', 'decod_conv3.1.weight', 'decod_conv3.1.bias', 'decod_conv4.1.weight', 'decod_conv4.1.bias', 'decod_conv5.1.weight', 'decod_conv5.1.bias', 'decod_conv6.1.weight', 'decod_conv6.1.bias'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch_weights['state_dict'].keys())\n",
    "weight_dict = torch_weights['state_dict']\n",
    "model_torch.load_state_dict(torch_weights['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the weights in the layers setting the read Pytorch weights to the Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_type = np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encod_conv1\n",
    "weight_bias_list = [weight_dict['encod_conv1.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['encod_conv1.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('encoder').encod_conv1.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# encod_conv2\n",
    "weight_bias_list = [weight_dict['encod_conv2.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['encod_conv2.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('encoder').encod_conv2.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# encod_conv3\n",
    "weight_bias_list = [weight_dict['encod_conv3.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['encod_conv3.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('encoder').encod_conv3.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# encod_conv4\n",
    "weight_bias_list = [weight_dict['encod_conv4.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['encod_conv4.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('encoder').encod_conv4.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# encod_conv5\n",
    "weight_bias_list = [weight_dict['encod_conv5.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['encod_conv5.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('encoder').encod_conv5.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# encod_conv6\n",
    "weight_bias_list = [weight_dict['encod_conv6.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['encod_conv6.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('encoder').encod_conv6.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# fc1\n",
    "weight_bias_list = [weight_dict['fc1.weight'].numpy().transpose(1, 0).astype(data_type),\n",
    "               weight_dict['fc1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('encoder').fc1.set_weights(weight_bias_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc2\n",
    "weight_bias_list = [weight_dict['fc2.weight'].numpy().transpose(1, 0).astype(data_type),\n",
    "               weight_dict['fc2.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('decoder').fc2.set_weights(weight_bias_list)\n",
    "\n",
    "# decod_conv1\n",
    "weight_bias_list = [weight_dict['decod_conv1.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['decod_conv1.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('decoder').decod_conv1.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# decod_conv2\n",
    "weight_bias_list = [weight_dict['decod_conv2.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['decod_conv2.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('decoder').decod_conv2.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# decod_conv3\n",
    "weight_bias_list = [weight_dict['decod_conv3.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['decod_conv3.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('decoder').decod_conv3.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# decod_conv4\n",
    "weight_bias_list = [weight_dict['decod_conv4.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['decod_conv4.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('decoder').decod_conv4.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# decod_conv5\n",
    "weight_bias_list = [weight_dict['decod_conv5.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['decod_conv5.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('decoder').decod_conv5.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# decod_conv6\n",
    "weight_bias_list = [weight_dict['decod_conv6.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['decod_conv6.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('decoder').decod_conv6.layers[1].set_weights(weight_bias_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that the weights are equal in both models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encod_conv1 weights: True\n",
      "Encod_conv1 biases: True\n",
      "Encod_conv2 weights: True\n",
      "Encod_conv2 biases: True\n",
      "Encod_conv3 weights: True\n",
      "Encod_conv3 biases: True\n",
      "Encod_conv4 weights: True\n",
      "Encod_conv4 biases: True\n",
      "Encod_conv5 weights: True\n",
      "Encod_conv5 biases: True\n",
      "Encod_conv6 weights: True\n",
      "Encod_conv6 biases: True\n",
      "FC1 weights: True\n",
      "FC1 biases: True\n"
     ]
    }
   ],
   "source": [
    "# encod_conv1 weights\n",
    "print(f\"Encod_conv1 weights: {np.all(model_keras.model.get_layer('encoder').encod_conv1.layers[1].get_weights()[0] == model_torch.encod_conv1[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# encod_conv1 biases\n",
    "print(f\"Encod_conv1 biases: {np.all(model_keras.model.get_layer('encoder').encod_conv1.layers[1].get_weights()[1] == model_torch.encod_conv1[1].bias.detach().numpy())}\")\n",
    "\n",
    "# encod_conv2 weights\n",
    "print(f\"Encod_conv2 weights: {np.all(model_keras.model.get_layer('encoder').encod_conv2.layers[1].get_weights()[0] == model_torch.encod_conv2[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# encod_conv2 biases\n",
    "print(f\"Encod_conv2 biases: {np.all(model_keras.model.get_layer('encoder').encod_conv2.layers[1].get_weights()[1] == model_torch.encod_conv2[1].bias.detach().numpy())}\")\n",
    "\n",
    "# encod_conv3 weights\n",
    "print(f\"Encod_conv3 weights: {np.all(model_keras.model.get_layer('encoder').encod_conv3.layers[1].get_weights()[0] == model_torch.encod_conv3[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# encod_conv3 biases\n",
    "print(f\"Encod_conv3 biases: {np.all(model_keras.model.get_layer('encoder').encod_conv3.layers[1].get_weights()[1] == model_torch.encod_conv3[1].bias.detach().numpy())}\")\n",
    "\n",
    "# encod_conv4 weights\n",
    "print(f\"Encod_conv4 weights: {np.all(model_keras.model.get_layer('encoder').encod_conv4.layers[1].get_weights()[0] == model_torch.encod_conv4[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# encod_conv4 biases\n",
    "print(f\"Encod_conv4 biases: {np.all(model_keras.model.get_layer('encoder').encod_conv4.layers[1].get_weights()[1] == model_torch.encod_conv4[1].bias.detach().numpy())}\")\n",
    "\n",
    "# encod_conv5 weights\n",
    "print(f\"Encod_conv5 weights: {np.all(model_keras.model.get_layer('encoder').encod_conv5.layers[1].get_weights()[0] == model_torch.encod_conv5[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# encod_conv5 biases\n",
    "print(f\"Encod_conv5 biases: {np.all(model_keras.model.get_layer('encoder').encod_conv5.layers[1].get_weights()[1] == model_torch.encod_conv5[1].bias.detach().numpy())}\")\n",
    "\n",
    "# encod_conv6 weights\n",
    "print(f\"Encod_conv6 weights: {np.all(model_keras.model.get_layer('encoder').encod_conv6.layers[1].get_weights()[0] == model_torch.encod_conv6[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# encod_conv6 biases\n",
    "print(f\"Encod_conv6 biases: {np.all(model_keras.model.get_layer('encoder').encod_conv6.layers[1].get_weights()[1] == model_torch.encod_conv6[1].bias.detach().numpy())}\")\n",
    "\n",
    "# fc1 weights\n",
    "print(f\"FC1 weights: {np.all(model_keras.model.get_layer('encoder').fc1.get_weights()[0] == model_torch.fc1.weight.detach().numpy().transpose(1, 0))}\")\n",
    "# fc1 biases\n",
    "print(f\"FC1 biases: {np.all(model_keras.model.get_layer('encoder').fc1.get_weights()[1] == model_torch.fc1.bias.detach().numpy())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC2 weights: True\n",
      "FC2 biases: True\n",
      "Decod_conv1 weights: True\n",
      "Decod_conv1 biases: True\n",
      "Decod_conv2 weights: True\n",
      "Decod_conv2 biases: True\n",
      "Decod_conv3 weights: True\n",
      "Decod_conv3 biases: True\n",
      "Decod_conv4 weights: True\n",
      "Decod_conv4 biases: True\n",
      "Decod_conv5 weights: True\n",
      "Decod_conv5 biases: True\n",
      "Decod_conv6 weights: True\n",
      "Decod_conv6 biases: True\n"
     ]
    }
   ],
   "source": [
    "# fc2 weights\n",
    "print(f\"FC2 weights: {np.all(model_keras.model.get_layer('decoder').fc2.get_weights()[0] == model_torch.fc2.weight.detach().numpy().transpose(1, 0))}\")\n",
    "# fc2 biases\n",
    "print(f\"FC2 biases: {np.all(model_keras.model.get_layer('decoder').fc2.get_weights()[1] == model_torch.fc2.bias.detach().numpy())}\")\n",
    "\n",
    "# decod_conv1 weights\n",
    "print(f\"Decod_conv1 weights: {np.all(model_keras.model.get_layer('decoder').decod_conv1.layers[1].get_weights()[0] == model_torch.decod_conv1[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# decod_conv1 biases\n",
    "print(f\"Decod_conv1 biases: {np.all(model_keras.model.get_layer('decoder').decod_conv1.layers[1].get_weights()[1] == model_torch.decod_conv1[1].bias.detach().numpy())}\")\n",
    "\n",
    "# decod_conv2 weights\n",
    "print(f\"Decod_conv2 weights: {np.all(model_keras.model.get_layer('decoder').decod_conv2.layers[1].get_weights()[0] == model_torch.decod_conv2[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# decod_conv2 biases\n",
    "print(f\"Decod_conv2 biases: {np.all(model_keras.model.get_layer('decoder').decod_conv2.layers[1].get_weights()[1] == model_torch.decod_conv2[1].bias.detach().numpy())}\")\n",
    "\n",
    "# decod_conv3 weights\n",
    "print(f\"Decod_conv3 weights: {np.all(model_keras.model.get_layer('decoder').decod_conv3.layers[1].get_weights()[0] == model_torch.decod_conv3[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# decod_conv3 biases\n",
    "print(f\"Decod_conv3 biases: {np.all(model_keras.model.get_layer('decoder').decod_conv3.layers[1].get_weights()[1] == model_torch.decod_conv3[1].bias.detach().numpy())}\")\n",
    "\n",
    "# decod_conv4 weights\n",
    "print(f\"Decod_conv4 weights: {np.all(model_keras.model.get_layer('decoder').decod_conv4.layers[1].get_weights()[0] == model_torch.decod_conv4[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# decod_conv4 biases\n",
    "print(f\"Decod_conv4 biases: {np.all(model_keras.model.get_layer('decoder').decod_conv4.layers[1].get_weights()[1] == model_torch.decod_conv4[1].bias.detach().numpy())}\")\n",
    "\n",
    "# decod_conv5 weights\n",
    "print(f\"Decod_conv5 weights: {np.all(model_keras.model.get_layer('decoder').decod_conv5.layers[1].get_weights()[0] == model_torch.decod_conv5[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# decod_conv5 biases\n",
    "print(f\"Decod_conv5 biases: {np.all(model_keras.model.get_layer('decoder').decod_conv5.layers[1].get_weights()[1] == model_torch.decod_conv5[1].bias.detach().numpy())}\")\n",
    "\n",
    "# decod_conv6 weights\n",
    "print(f\"Decod_conv6 weights: {np.all(model_keras.model.get_layer('decoder').decod_conv6.layers[1].get_weights()[0] == model_torch.decod_conv6[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# decod_conv6 biases\n",
    "print(f\"Decod_conv6 biases: {np.all(model_keras.model.get_layer('decoder').decod_conv6.layers[1].get_weights()[1] == model_torch.decod_conv6[1].bias.detach().numpy())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that both models give a very similar output for the same input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  60.697445\n",
      "diff max (3D):  18.677742\n",
      "diff norm (3D):  215.90654\n",
      "Outputs are similar:  False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "random_streamline = np.random.rand(1, 256, 3)\n",
    "dummy_input_keras = tf.convert_to_tensor(random_streamline, dtype=tf.float32)\n",
    "dummy_input_torch = torch.Tensor(random_streamline.transpose(0, 2, 1))\n",
    "\n",
    "output_keras = model_keras(dummy_input_keras)\n",
    "output_torch = model_torch(dummy_input_torch).detach().numpy()\n",
    "output_torch_reshaped = output_torch.transpose(0, 2, 1)\n",
    "\n",
    "print(\"MSE: \", np.mean((output_keras - output_torch_reshaped) ** 2))\n",
    "print(\"diff max (3D): \", np.max(output_keras - output_torch_reshaped))\n",
    "print(\"diff norm (3D): \", np.linalg.norm(output_keras - output_torch_reshaped))\n",
    "are_close = np.isclose(output_keras, output_torch_reshaped, atol=1e-6)\n",
    "\n",
    "# Check if all elements are close\n",
    "outputs_are_similar = np.all(are_close)\n",
    "print(\"Outputs are similar: \", outputs_are_similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs are not similar. Let's check where this happens in the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder Keras\n",
    "encoder = model_keras.model.get_layer('encoder')\n",
    "encoder_keras_layers = [getattr(encoder, layer) for layer in dir(encoder) if layer.startswith(('encod_conv', 'fc'))]\n",
    "# encoder Torch\n",
    "encoder_torch_layers = [getattr(model_torch, layer) for layer in dir(model_torch) if layer.startswith(('encod_conv', 'fc1'))]\n",
    "\n",
    "# decoder Keras\n",
    "decoder = model_keras.model.get_layer('decoder')\n",
    "decoder_keras_layers = [getattr(decoder, layer) for layer in dir(decoder) if layer.startswith(('decod_conv', 'fc'))]\n",
    "# decoder Torch\n",
    "decoder_torch_layers = [getattr(model_torch, layer) for layer in dir(model_torch) if layer.startswith(('decod_conv', 'fc2'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the conv1d layers and check if the output is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential_5, built=True>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_keras_layers[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER LAYER 1\n",
      "Keras: <Sequential name=sequential, built=True>\n",
      "Torch: Sequential(\n",
      "  (0): ReflectionPad1d((1, 1))\n",
      "  (1): Conv1d(3, 32, kernel_size=(3,), stride=(2,))\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In layer 0 Outputs are similar? True\n",
      "MSE is: 8.115592074232902e-17\n",
      "\n",
      "ENCODER LAYER 2\n",
      "Keras: <Sequential name=sequential_1, built=True>\n",
      "Torch: Sequential(\n",
      "  (0): ReflectionPad1d((1, 1))\n",
      "  (1): Conv1d(32, 64, kernel_size=(3,), stride=(2,))\n",
      ")\n",
      "In layer 1 Outputs are similar? True\n",
      "MSE is: 4.5786538017398616e-17\n",
      "\n",
      "ENCODER LAYER 3\n",
      "Keras: <Sequential name=sequential_2, built=True>\n",
      "Torch: Sequential(\n",
      "  (0): ReflectionPad1d((1, 1))\n",
      "  (1): Conv1d(64, 128, kernel_size=(3,), stride=(2,))\n",
      ")\n",
      "In layer 2 Outputs are similar? True\n",
      "MSE is: 7.266666703173203e-18\n",
      "\n",
      "ENCODER LAYER 4\n",
      "Keras: <Sequential name=sequential_3, built=True>\n",
      "Torch: Sequential(\n",
      "  (0): ReflectionPad1d((1, 1))\n",
      "  (1): Conv1d(128, 256, kernel_size=(3,), stride=(2,))\n",
      ")\n",
      "In layer 3 Outputs are similar? True\n",
      "MSE is: 3.997768863552458e-18\n",
      "\n",
      "ENCODER LAYER 5\n",
      "Keras: <Sequential name=sequential_4, built=True>\n",
      "Torch: Sequential(\n",
      "  (0): ReflectionPad1d((1, 1))\n",
      "  (1): Conv1d(256, 512, kernel_size=(3,), stride=(2,))\n",
      ")\n",
      "In layer 4 Outputs are similar? True\n",
      "MSE is: 6.499549949644335e-19\n",
      "\n",
      "ENCODER LAYER 6\n",
      "Keras: <Sequential name=sequential_5, built=True>\n",
      "Torch: Sequential(\n",
      "  (0): ReflectionPad1d((1, 1))\n",
      "  (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
      ")\n",
      "In layer 5 Outputs are similar? True\n",
      "MSE is: 9.439510526491784e-18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_streamline = np.random.rand(1, 256, 3)\n",
    "dummy_input_keras = tf.convert_to_tensor(random_streamline, dtype=tf.float32)\n",
    "dummy_input_torch = torch.Tensor(random_streamline.transpose(0, 2, 1))\n",
    "\n",
    "for i, (layer_keras, layer_torch) in enumerate(zip(encoder_keras_layers[:-1], encoder_torch_layers[:-1])):\n",
    "    print(f\"ENCODER LAYER {i+1}\")\n",
    "    print(f\"Keras: {layer_keras}\")\n",
    "    print(f\"Torch: {layer_torch}\")\n",
    "    # Run the input through the layers\n",
    "    layer_output_keras = dummy_input_keras\n",
    "    layer_output_torch = dummy_input_torch\n",
    "    for c in range(i+1):\n",
    "        if c == 5:  # conv6, do not ReLU\n",
    "            layer_output_keras = encoder_keras_layers[c](layer_output_keras)\n",
    "            layer_output_torch = encoder_torch_layers[c](layer_output_torch)\n",
    "        else:\n",
    "            layer_output_keras = tf.nn.relu(encoder_keras_layers[c](layer_output_keras))\n",
    "            layer_output_torch = F.relu(encoder_torch_layers[c](layer_output_torch))\n",
    "    # Check if the outputs are similar\n",
    "    layer_output_torch_reshaped = layer_output_torch.detach().numpy().transpose(0, 2, 1)\n",
    "    are_close = np.all(np.isclose(layer_output_keras, layer_output_torch_reshaped, atol=1e-6))\n",
    "    print(f\"In layer {i} Outputs are similar? {are_close}\")\n",
    "    print(f\"MSE is: {np.mean((layer_output_keras - layer_output_torch_reshaped) ** 2)}\\n\")\n",
    "\n",
    "\n",
    "# reshape before running into fc1\n",
    "encoder_out_size_keras = (layer_output_keras.shape[1], layer_output_keras.shape[2])\n",
    "h7_keras = tf.reshape(layer_output_keras, (-1, encoder_out_size_keras[0] * encoder_out_size_keras[1]))\n",
    "\n",
    "encoder_out_size_torch = (layer_output_torch.shape[1], layer_output_torch.shape[2])\n",
    "h7_torch = layer_output_torch.view(-1, encoder_out_size_torch[0] * encoder_out_size_torch[1]).detach().numpy()\n",
    "\n",
    "# run through fc1\n",
    "\n",
    "are_close = np.all(np.isclose(h7_keras, h7_torch, atol=1e-6))\n",
    "print(f\"In layer {7} Outputs are similar? {are_close}\")\n",
    "print(f\"MSE is: {np.mean((h7_keras - h7_torch) ** 2)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check if the fc1 layer output is similar too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# reshape the output before passing it to fc1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff max (3D):  8.731149137020111e-11\n",
      "diff norm (3D):  3.8385555479116074e-09\n",
      "Outputs are similar:  True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "data_type = np.float64\n",
    "\n",
    "# Pytorch 3D Convolution with 1 input channel and 32 output channels\n",
    "# Initialize weights for PyTorch model\n",
    "# Shape for PyTorch Conv3d: (out_channels, in_channels, D, H, W)\n",
    "weight_pytorch = 10.12 * np.random.randint(1, 1000, (32, 1, 3, 3, 3)).astype(data_type)  \n",
    "weights = torch.from_numpy(weight_pytorch)\n",
    "biases = torch.zeros(32, dtype=torch.float64)  # Biases for 32 output channels\n",
    "\n",
    "inputs_torch = torch.from_numpy(1.5 * np.ones((1, 1, 10, 10, 10), dtype=data_type))  \n",
    "torch_model_3d = nn.Conv3d(1, 32, kernel_size=3, stride=1, padding=1)  \n",
    "torch_model_3d.weight = nn.Parameter(weights)\n",
    "torch_model_3d.bias = nn.Parameter(biases)\n",
    "torch_output_3d = torch_model_3d(inputs_torch)\n",
    "\n",
    "# Convert PyTorch weights for TensorFlow\n",
    "weight_tf = weight_pytorch.transpose((2, 3, 4, 1, 0))  # Reorder dimensions for TensorFlow: (D, H, W, in_channels, out_channels)\n",
    "\n",
    "inputs = tf.Variable(1.5 * np.ones((1, 10, 10, 10, 1), dtype=data_type)) \n",
    "\n",
    "# TensorFlow 3D Convolution with 1 input channel and 32 output channels\n",
    "conv3d_layer = tf.keras.layers.Conv3D(32, [3, 3, 3], strides=(1, 1, 1), padding='same',\n",
    "                                      kernel_initializer=tf.constant_initializer(weight_tf),\n",
    "                                      bias_initializer=tf.constant_initializer(0),\n",
    "                                      activation=None, dtype=data_type)\n",
    "\n",
    "# Apply the 3D convolution operation\n",
    "tf_output_3d = conv3d_layer(inputs)\n",
    "\n",
    "# Compare results for 3D Convolution with 1 input channel and 32 output channels\n",
    "# Ensure TensorFlow output is converted to NumPy for comparison\n",
    "tf_output_3d_numpy = tf_output_3d.numpy()\n",
    "torch_output_3d_numpy = torch_output_3d.permute((0, 2, 3, 4, 1)).detach().numpy()  \n",
    "\n",
    "print(\"diff max (3D): \", np.max(tf_output_3d_numpy - torch_output_3d_numpy))\n",
    "print(\"diff norm (3D): \", np.linalg.norm(tf_output_3d_numpy - torch_output_3d_numpy))\n",
    "are_close = np.isclose(tf_output_3d_numpy, torch_output_3d_numpy, atol=1e-6)\n",
    "\n",
    "# Check if all elements are close\n",
    "outputs_are_similar = np.all(are_close)\n",
    "print(\"Outputs are similar: \", outputs_are_similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to load the weights from the HDF5 file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
