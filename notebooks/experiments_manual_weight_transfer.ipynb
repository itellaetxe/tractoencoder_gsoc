{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the PyTorch model as it is in Tractolearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class AETorch(nn.Module):\n",
    "    \"\"\"Strided convolution-upsampling-based AE using reflection-padding and\n",
    "    increasing feature maps in decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_space_dims):\n",
    "        super(AETorch, self).__init__()\n",
    "\n",
    "        self.kernel_size = 3\n",
    "        self.latent_space_dims = latent_space_dims\n",
    "\n",
    "        self.pad = nn.ReflectionPad1d(1)\n",
    "\n",
    "        def pre_pad(m):\n",
    "            return nn.Sequential(self.pad, m)\n",
    "\n",
    "        self.encod_conv1 = pre_pad(\n",
    "            nn.Conv1d(3, 32, self.kernel_size, stride=2, padding=0)\n",
    "        )\n",
    "        self.encod_conv2 = pre_pad(\n",
    "            nn.Conv1d(32, 64, self.kernel_size, stride=2, padding=0)\n",
    "        )\n",
    "        self.encod_conv3 = pre_pad(\n",
    "            nn.Conv1d(64, 128, self.kernel_size, stride=2, padding=0)\n",
    "        )\n",
    "        self.encod_conv4 = pre_pad(\n",
    "            nn.Conv1d(128, 256, self.kernel_size, stride=2, padding=0)\n",
    "        )\n",
    "        self.encod_conv5 = pre_pad(\n",
    "            nn.Conv1d(256, 512, self.kernel_size, stride=2, padding=0)\n",
    "        )\n",
    "        self.encod_conv6 = pre_pad(\n",
    "            nn.Conv1d(512, 1024, self.kernel_size, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(8192, self.latent_space_dims)  # 8192 = 1024*8\n",
    "        self.fc2 = nn.Linear(self.latent_space_dims, 8192)\n",
    "\n",
    "        self.decod_conv1 = pre_pad(\n",
    "            nn.Conv1d(1024, 512, self.kernel_size, stride=1, padding=0)\n",
    "        )\n",
    "        self.upsampl1 = nn.Upsample(\n",
    "            scale_factor=2, mode=\"linear\", align_corners=False\n",
    "        )\n",
    "        self.decod_conv2 = pre_pad(\n",
    "            nn.Conv1d(512, 256, self.kernel_size, stride=1, padding=0)\n",
    "        )\n",
    "        self.upsampl2 = nn.Upsample(\n",
    "            scale_factor=2, mode=\"linear\", align_corners=False\n",
    "        )\n",
    "        self.decod_conv3 = pre_pad(\n",
    "            nn.Conv1d(256, 128, self.kernel_size, stride=1, padding=0)\n",
    "        )\n",
    "        self.upsampl3 = nn.Upsample(\n",
    "            scale_factor=2, mode=\"linear\", align_corners=False\n",
    "        )\n",
    "        self.decod_conv4 = pre_pad(\n",
    "            nn.Conv1d(128, 64, self.kernel_size, stride=1, padding=0)\n",
    "        )\n",
    "        self.upsampl4 = nn.Upsample(\n",
    "            scale_factor=2, mode=\"linear\", align_corners=False\n",
    "        )\n",
    "        self.decod_conv5 = pre_pad(\n",
    "            nn.Conv1d(64, 32, self.kernel_size, stride=1, padding=0)\n",
    "        )\n",
    "        self.upsampl5 = nn.Upsample(\n",
    "            scale_factor=2, mode=\"linear\", align_corners=False\n",
    "        )\n",
    "        self.decod_conv6 = pre_pad(\n",
    "            nn.Conv1d(32, 3, self.kernel_size, stride=1, padding=0)\n",
    "        )\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.encod_conv1(x))\n",
    "        h2 = F.relu(self.encod_conv2(h1))\n",
    "        h3 = F.relu(self.encod_conv3(h2))\n",
    "        h4 = F.relu(self.encod_conv4(h3))\n",
    "        h5 = F.relu(self.encod_conv5(h4))\n",
    "        h6 = self.encod_conv6(h5)\n",
    "\n",
    "        self.encoder_out_size = (h6.shape[1], h6.shape[2])\n",
    "\n",
    "        # Flatten\n",
    "        h7 = h6.view(-1, self.encoder_out_size[0] * self.encoder_out_size[1])\n",
    "\n",
    "        fc1 = self.fc1(h7)\n",
    "\n",
    "        return fc1\n",
    "\n",
    "    def decode(self, z):\n",
    "        fc = self.fc2(z)\n",
    "        fc_reshape = fc.view(\n",
    "            -1, self.encoder_out_size[0], self.encoder_out_size[1]\n",
    "        )\n",
    "        h1 = F.relu(self.decod_conv1(fc_reshape))\n",
    "        h2 = self.upsampl1(h1)\n",
    "        h3 = F.relu(self.decod_conv2(h2))\n",
    "        h4 = self.upsampl2(h3)\n",
    "        h5 = F.relu(self.decod_conv3(h4))\n",
    "        h6 = self.upsampl3(h5)\n",
    "        h7 = F.relu(self.decod_conv4(h6))\n",
    "        h8 = self.upsampl4(h7)\n",
    "        h9 = F.relu(self.decod_conv5(h8))\n",
    "        h10 = self.upsampl5(h9)\n",
    "        h11 = self.decod_conv6(h10)\n",
    "\n",
    "        return h11\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encode(x)\n",
    "        return self.decode(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 11:35:15.007344: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-20 11:35:15.044821: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-20 11:35:15.854026: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tractoencoder_gsoc.ae_model import IncrFeatStridedConvFCUpsampReflectPadAE as AEKeras\n",
    "model_torch = AETorch(32)\n",
    "model_keras = AEKeras(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the shape of the weights and biases in some layers between Torch and Keras to see the difference pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER LAYER CONV6\n",
      "Weights TORCH: torch.Size([1024, 512, 3])\n",
      "Biases TORCH: torch.Size([1024])\n",
      "---------------------------------\n",
      "Weights KERAS: (3, 512, 1024)\n",
      "Biases KERAS: (1024,)\n",
      "\n",
      "\n",
      "ENCODER LAYER CONV3\n",
      "Weights TORCH: torch.Size([128, 64, 3])\n",
      "Biases TORCH: torch.Size([128])\n",
      "---------------------------------\n",
      "Weights KERAS: (3, 64, 128)\n",
      "Biases KERAS: (128,)\n",
      "\n",
      "\n",
      "DECODER LAYER FC1\n",
      "Weights TORCH: torch.Size([32, 8192])\n",
      "Biases TORCH: torch.Size([32])\n",
      "---------------------------------\n",
      "Weights KERAS: (8192, 32)\n",
      "Biases KERAS: (32,)\n",
      "\n",
      "\n",
      "DECODER LAYER CONV1\n",
      "Weights TORCH: torch.Size([512, 1024, 3])\n",
      "Biases TORCH: torch.Size([512])\n",
      "---------------------------------\n",
      "Weights KERAS: (3, 1024, 512)\n",
      "Biases KERAS: (512,)\n"
     ]
    }
   ],
   "source": [
    "print(\"ENCODER LAYER CONV6\")\n",
    "print(f\"Weights TORCH: {model_torch.encod_conv6[1].weight.shape}\")\n",
    "print(f\"Biases TORCH: {model_torch.encod_conv6[1].bias.shape}\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Weights KERAS: {model_keras.model.get_layer('encoder').encod_conv6.layers[1].get_weights()[0].shape}\")\n",
    "print(f\"Biases KERAS: {model_keras.model.get_layer('encoder').encod_conv6.layers[1].get_weights()[1].shape}\")\n",
    "\n",
    "print(\"\\n\\nENCODER LAYER CONV3\")\n",
    "print(f\"Weights TORCH: {model_torch.encod_conv3[1].weight.shape}\")\n",
    "print(f\"Biases TORCH: {model_torch.encod_conv3[1].bias.shape}\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Weights KERAS: {model_keras.model.get_layer('encoder').encod_conv3.layers[1].get_weights()[0].shape}\")\n",
    "print(f\"Biases KERAS: {model_keras.model.get_layer('encoder').encod_conv3.layers[1].get_weights()[1].shape}\")\n",
    "\n",
    "print(\"\\n\\nDECODER LAYER FC1\")\n",
    "print(f\"Weights TORCH: {model_torch.fc1.weight.shape}\")\n",
    "print(f\"Biases TORCH: {model_torch.fc1.bias.shape}\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Weights KERAS: {model_keras.model.get_layer('encoder').fc1.get_weights()[0].shape}\")\n",
    "print(f\"Biases KERAS: {model_keras.model.get_layer('encoder').fc1.get_weights()[1].shape}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nDECODER LAYER CONV1\")\n",
    "print(f\"Weights TORCH: {model_torch.decod_conv1[1].weight.shape}\")\n",
    "print(f\"Biases TORCH: {model_torch.decod_conv1[1].bias.shape}\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Weights KERAS: {model_keras.model.get_layer('decoder').decod_conv1.layers[1].get_weights()[0].shape}\")\n",
    "print(f\"Biases KERAS: {model_keras.model.get_layer('decoder').decod_conv1.layers[1].get_weights()[1].shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ WEIGHTS FROM PTH FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "pth_path = os.path.abspath(\"/home/teitxe/data/fibercup_model/best_model_fibercup.pt\")\n",
    "torch_weights = torch.load(pth_path, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'state_dict', 'lowest_loss', 'optimizer'])\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "print(torch_weights.keys())\n",
    "print(torch_weights['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['encod_conv1.1.weight', 'encod_conv1.1.bias', 'encod_conv2.1.weight', 'encod_conv2.1.bias', 'encod_conv3.1.weight', 'encod_conv3.1.bias', 'encod_conv4.1.weight', 'encod_conv4.1.bias', 'encod_conv5.1.weight', 'encod_conv5.1.bias', 'encod_conv6.1.weight', 'encod_conv6.1.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'decod_conv1.1.weight', 'decod_conv1.1.bias', 'decod_conv2.1.weight', 'decod_conv2.1.bias', 'decod_conv3.1.weight', 'decod_conv3.1.bias', 'decod_conv4.1.weight', 'decod_conv4.1.bias', 'decod_conv5.1.weight', 'decod_conv5.1.bias', 'decod_conv6.1.weight', 'decod_conv6.1.bias'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch_weights['state_dict'].keys())\n",
    "weight_dict = torch_weights['state_dict']\n",
    "model_torch.load_state_dict(torch_weights['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the weights in the layers setting the read Pytorch weights to the Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_type = np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encod_conv1\n",
    "weight_bias_list = [weight_dict['encod_conv1.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['encod_conv1.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('encoder').encod_conv1.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# encod_conv2\n",
    "weight_bias_list = [weight_dict['encod_conv2.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['encod_conv2.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('encoder').encod_conv2.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# encod_conv3\n",
    "weight_bias_list = [weight_dict['encod_conv3.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['encod_conv3.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('encoder').encod_conv3.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# encod_conv4\n",
    "weight_bias_list = [weight_dict['encod_conv4.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['encod_conv4.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('encoder').encod_conv4.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# encod_conv5\n",
    "weight_bias_list = [weight_dict['encod_conv5.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['encod_conv5.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('encoder').encod_conv5.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# encod_conv6\n",
    "weight_bias_list = [weight_dict['encod_conv6.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['encod_conv6.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('encoder').encod_conv6.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# fc1\n",
    "weight_bias_list = [weight_dict['fc1.weight'].numpy().transpose(1, 0).astype(data_type),\n",
    "               weight_dict['fc1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('encoder').fc1.set_weights(weight_bias_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc2\n",
    "weight_bias_list = [weight_dict['fc2.weight'].numpy().transpose(1, 0).astype(data_type),\n",
    "               weight_dict['fc2.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('decoder').fc2.set_weights(weight_bias_list)\n",
    "\n",
    "# decod_conv1\n",
    "weight_bias_list = [weight_dict['decod_conv1.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['decod_conv1.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('decoder').decod_conv1.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# decod_conv2\n",
    "weight_bias_list = [weight_dict['decod_conv2.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['decod_conv2.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('decoder').decod_conv2.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# decod_conv3\n",
    "weight_bias_list = [weight_dict['decod_conv3.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['decod_conv3.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('decoder').decod_conv3.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# decod_conv4\n",
    "weight_bias_list = [weight_dict['decod_conv4.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['decod_conv4.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('decoder').decod_conv4.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# decod_conv5\n",
    "weight_bias_list = [weight_dict['decod_conv5.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['decod_conv5.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('decoder').decod_conv5.layers[1].set_weights(weight_bias_list)\n",
    "\n",
    "# decod_conv6\n",
    "weight_bias_list = [weight_dict['decod_conv6.1.weight'].numpy().transpose(2, 1, 0).astype(data_type),\n",
    "               weight_dict['decod_conv6.1.bias'].numpy().astype(data_type)]\n",
    "model_keras.model.get_layer('decoder').decod_conv6.layers[1].set_weights(weight_bias_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that the weights are equal in both models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encod_conv1 weights: True\n",
      "Encod_conv1 biases: True\n",
      "Encod_conv2 weights: True\n",
      "Encod_conv2 biases: True\n",
      "Encod_conv3 weights: True\n",
      "Encod_conv3 biases: True\n",
      "Encod_conv4 weights: True\n",
      "Encod_conv4 biases: True\n",
      "Encod_conv5 weights: True\n",
      "Encod_conv5 biases: True\n",
      "Encod_conv6 weights: True\n",
      "Encod_conv6 biases: True\n",
      "FC1 weights: True\n",
      "FC1 biases: True\n"
     ]
    }
   ],
   "source": [
    "# encod_conv1 weights\n",
    "print(f\"Encod_conv1 weights: {np.all(model_keras.model.get_layer('encoder').encod_conv1.layers[1].get_weights()[0] == model_torch.encod_conv1[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# encod_conv1 biases\n",
    "print(f\"Encod_conv1 biases: {np.all(model_keras.model.get_layer('encoder').encod_conv1.layers[1].get_weights()[1] == model_torch.encod_conv1[1].bias.detach().numpy())}\")\n",
    "\n",
    "# encod_conv2 weights\n",
    "print(f\"Encod_conv2 weights: {np.all(model_keras.model.get_layer('encoder').encod_conv2.layers[1].get_weights()[0] == model_torch.encod_conv2[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# encod_conv2 biases\n",
    "print(f\"Encod_conv2 biases: {np.all(model_keras.model.get_layer('encoder').encod_conv2.layers[1].get_weights()[1] == model_torch.encod_conv2[1].bias.detach().numpy())}\")\n",
    "\n",
    "# encod_conv3 weights\n",
    "print(f\"Encod_conv3 weights: {np.all(model_keras.model.get_layer('encoder').encod_conv3.layers[1].get_weights()[0] == model_torch.encod_conv3[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# encod_conv3 biases\n",
    "print(f\"Encod_conv3 biases: {np.all(model_keras.model.get_layer('encoder').encod_conv3.layers[1].get_weights()[1] == model_torch.encod_conv3[1].bias.detach().numpy())}\")\n",
    "\n",
    "# encod_conv4 weights\n",
    "print(f\"Encod_conv4 weights: {np.all(model_keras.model.get_layer('encoder').encod_conv4.layers[1].get_weights()[0] == model_torch.encod_conv4[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# encod_conv4 biases\n",
    "print(f\"Encod_conv4 biases: {np.all(model_keras.model.get_layer('encoder').encod_conv4.layers[1].get_weights()[1] == model_torch.encod_conv4[1].bias.detach().numpy())}\")\n",
    "\n",
    "# encod_conv5 weights\n",
    "print(f\"Encod_conv5 weights: {np.all(model_keras.model.get_layer('encoder').encod_conv5.layers[1].get_weights()[0] == model_torch.encod_conv5[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# encod_conv5 biases\n",
    "print(f\"Encod_conv5 biases: {np.all(model_keras.model.get_layer('encoder').encod_conv5.layers[1].get_weights()[1] == model_torch.encod_conv5[1].bias.detach().numpy())}\")\n",
    "\n",
    "# encod_conv6 weights\n",
    "print(f\"Encod_conv6 weights: {np.all(model_keras.model.get_layer('encoder').encod_conv6.layers[1].get_weights()[0] == model_torch.encod_conv6[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# encod_conv6 biases\n",
    "print(f\"Encod_conv6 biases: {np.all(model_keras.model.get_layer('encoder').encod_conv6.layers[1].get_weights()[1] == model_torch.encod_conv6[1].bias.detach().numpy())}\")\n",
    "\n",
    "# fc1 weights\n",
    "print(f\"FC1 weights: {np.all(model_keras.model.get_layer('encoder').fc1.get_weights()[0] == model_torch.fc1.weight.detach().numpy().transpose(1, 0))}\")\n",
    "# fc1 biases\n",
    "print(f\"FC1 biases: {np.all(model_keras.model.get_layer('encoder').fc1.get_weights()[1] == model_torch.fc1.bias.detach().numpy())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC2 weights: True\n",
      "FC2 biases: True\n",
      "Decod_conv1 weights: True\n",
      "Decod_conv1 biases: True\n",
      "Decod_conv2 weights: True\n",
      "Decod_conv2 biases: True\n",
      "Decod_conv3 weights: True\n",
      "Decod_conv3 biases: True\n",
      "Decod_conv4 weights: True\n",
      "Decod_conv4 biases: True\n",
      "Decod_conv5 weights: True\n",
      "Decod_conv5 biases: True\n",
      "Decod_conv6 weights: True\n",
      "Decod_conv6 biases: True\n"
     ]
    }
   ],
   "source": [
    "# fc2 weights\n",
    "print(f\"FC2 weights: {np.all(model_keras.model.get_layer('decoder').fc2.get_weights()[0] == model_torch.fc2.weight.detach().numpy().transpose(1, 0))}\")\n",
    "# fc2 biases\n",
    "print(f\"FC2 biases: {np.all(model_keras.model.get_layer('decoder').fc2.get_weights()[1] == model_torch.fc2.bias.detach().numpy())}\")\n",
    "\n",
    "# decod_conv1 weights\n",
    "print(f\"Decod_conv1 weights: {np.all(model_keras.model.get_layer('decoder').decod_conv1.layers[1].get_weights()[0] == model_torch.decod_conv1[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# decod_conv1 biases\n",
    "print(f\"Decod_conv1 biases: {np.all(model_keras.model.get_layer('decoder').decod_conv1.layers[1].get_weights()[1] == model_torch.decod_conv1[1].bias.detach().numpy())}\")\n",
    "\n",
    "# decod_conv2 weights\n",
    "print(f\"Decod_conv2 weights: {np.all(model_keras.model.get_layer('decoder').decod_conv2.layers[1].get_weights()[0] == model_torch.decod_conv2[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# decod_conv2 biases\n",
    "print(f\"Decod_conv2 biases: {np.all(model_keras.model.get_layer('decoder').decod_conv2.layers[1].get_weights()[1] == model_torch.decod_conv2[1].bias.detach().numpy())}\")\n",
    "\n",
    "# decod_conv3 weights\n",
    "print(f\"Decod_conv3 weights: {np.all(model_keras.model.get_layer('decoder').decod_conv3.layers[1].get_weights()[0] == model_torch.decod_conv3[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# decod_conv3 biases\n",
    "print(f\"Decod_conv3 biases: {np.all(model_keras.model.get_layer('decoder').decod_conv3.layers[1].get_weights()[1] == model_torch.decod_conv3[1].bias.detach().numpy())}\")\n",
    "\n",
    "# decod_conv4 weights\n",
    "print(f\"Decod_conv4 weights: {np.all(model_keras.model.get_layer('decoder').decod_conv4.layers[1].get_weights()[0] == model_torch.decod_conv4[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# decod_conv4 biases\n",
    "print(f\"Decod_conv4 biases: {np.all(model_keras.model.get_layer('decoder').decod_conv4.layers[1].get_weights()[1] == model_torch.decod_conv4[1].bias.detach().numpy())}\")\n",
    "\n",
    "# decod_conv5 weights\n",
    "print(f\"Decod_conv5 weights: {np.all(model_keras.model.get_layer('decoder').decod_conv5.layers[1].get_weights()[0] == model_torch.decod_conv5[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# decod_conv5 biases\n",
    "print(f\"Decod_conv5 biases: {np.all(model_keras.model.get_layer('decoder').decod_conv5.layers[1].get_weights()[1] == model_torch.decod_conv5[1].bias.detach().numpy())}\")\n",
    "\n",
    "# decod_conv6 weights\n",
    "print(f\"Decod_conv6 weights: {np.all(model_keras.model.get_layer('decoder').decod_conv6.layers[1].get_weights()[0] == model_torch.decod_conv6[1].weight.detach().numpy().transpose(2, 1, 0))}\")\n",
    "# decod_conv6 biases\n",
    "print(f\"Decod_conv6 biases: {np.all(model_keras.model.get_layer('decoder').decod_conv6.layers[1].get_weights()[1] == model_torch.decod_conv6[1].bias.detach().numpy())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that both models give a very similar output for the same input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  4637.233\n",
      "diff max (3D):  162.8513\n",
      "diff norm (3D):  105273.695\n",
      "Outputs are similar:  False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tractoencoder_gsoc import utils\n",
    "trk = os.path.abspath(\"../data/fibercup/fibercup_advanced_filtering_no_ushapes/ae_input_std_endpoints/train/fibercup_Simulated_prob_tracking_minL10_resampled256_plausibles_std_endpoints_train.trk\")\n",
    "anat = os.path.abspath(\"../data/fibercup/Simulated_FiberCup.nii.gz\")\n",
    "random_streamlines = utils.prepare_tensor_from_file(trk, anat).numpy()\n",
    "dummy_input_keras = tf.convert_to_tensor(random_streamlines, dtype=tf.float32)\n",
    "dummy_input_torch = torch.Tensor(random_streamlines.transpose(0, 2, 1))\n",
    "\n",
    "output_keras = model_keras(dummy_input_keras)\n",
    "output_torch = model_torch(dummy_input_torch).detach().numpy()\n",
    "output_torch_reshaped = output_torch.transpose(0, 2, 1)\n",
    "\n",
    "print(\"MSE: \", np.mean((output_keras - output_torch_reshaped) ** 2))\n",
    "print(\"diff max (3D): \", np.max(output_keras - output_torch_reshaped))\n",
    "print(\"diff norm (3D): \", np.linalg.norm(output_keras - output_torch_reshaped))\n",
    "are_close = np.isclose(output_keras, output_torch_reshaped, atol=1e-6)\n",
    "\n",
    "# Check if all elements are close\n",
    "outputs_are_similar = np.all(are_close)\n",
    "print(\"Outputs are similar: \", outputs_are_similar)\n",
    "\n",
    "# Save the output tractograms\n",
    "data_wd = os.path.abspath(\"./data_outputs/model_comparison/\")\n",
    "os.makedirs(data_wd, exist_ok=True)\n",
    "output_fname_keras = os.path.join(data_wd,\n",
    "                                  \"output_keras.trk\")\n",
    "utils.save_tractogram(output_keras, output_fname_keras)\n",
    "output_fname_torch = os.path.join(data_wd,\n",
    "                                  \"output_torch.trk\")\n",
    "utils.save_tractogram(output_torch_reshaped, output_fname_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs are not similar. Let's check where this happens in the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder Keras\n",
    "encoder = model_keras.model.get_layer('encoder')\n",
    "encoder_keras_layers = [getattr(encoder, layer) for layer in dir(encoder) if layer.startswith(('encod_conv', 'fc'))]\n",
    "# encoder Torch\n",
    "encoder_torch_layers = [getattr(model_torch, layer) for layer in dir(model_torch) if layer.startswith(('encod_conv', 'fc1'))]\n",
    "\n",
    "# decoder Keras\n",
    "decoder = model_keras.model.get_layer('decoder')\n",
    "decoder_keras_layers = [getattr(decoder, layer) for layer in dir(decoder) if layer.startswith(('decod_conv', 'fc'))]\n",
    "# decoder Torch\n",
    "decoder_torch_layers = [getattr(model_torch, layer) for layer in dir(model_torch) if layer.startswith(('decod_conv', 'fc2'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the conv1d layers and check if the output is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER LAYER 1\n",
      "Keras: <Sequential name=sequential, built=True>\n",
      "Torch: Sequential(\n",
      "  (0): ReflectionPad1d((1, 1))\n",
      "  (1): Conv1d(3, 32, kernel_size=(3,), stride=(2,))\n",
      ")\n",
      "In layer 1 Outputs are similar? FALSE\n",
      "MAE is: 3.2603165323052963e-07\n",
      "MSE is: 1.1156285313965175e-12\n",
      "\n",
      "ENCODER LAYER 2\n",
      "Keras: <Sequential name=sequential_1, built=True>\n",
      "Torch: Sequential(\n",
      "  (0): ReflectionPad1d((1, 1))\n",
      "  (1): Conv1d(32, 64, kernel_size=(3,), stride=(2,))\n",
      ")\n",
      "In layer 2 Outputs are similar? FALSE\n",
      "MAE is: 1.8141469126931042e-07\n",
      "MSE is: 3.846853120336585e-13\n",
      "\n",
      "ENCODER LAYER 3\n",
      "Keras: <Sequential name=sequential_2, built=True>\n",
      "Torch: Sequential(\n",
      "  (0): ReflectionPad1d((1, 1))\n",
      "  (1): Conv1d(64, 128, kernel_size=(3,), stride=(2,))\n",
      ")\n",
      "In layer 3 Outputs are similar? FALSE\n",
      "MAE is: 3.93934698195153e-08\n",
      "MSE is: 6.669323974377653e-14\n",
      "\n",
      "ENCODER LAYER 4\n",
      "Keras: <Sequential name=sequential_3, built=True>\n",
      "Torch: Sequential(\n",
      "  (0): ReflectionPad1d((1, 1))\n",
      "  (1): Conv1d(128, 256, kernel_size=(3,), stride=(2,))\n",
      ")\n",
      "In layer 4 Outputs are similar? FALSE\n",
      "MAE is: 2.5869125508393154e-08\n",
      "MSE is: 1.1190516490343881e-14\n",
      "\n",
      "ENCODER LAYER 5\n",
      "Keras: <Sequential name=sequential_4, built=True>\n",
      "Torch: Sequential(\n",
      "  (0): ReflectionPad1d((1, 1))\n",
      "  (1): Conv1d(256, 512, kernel_size=(3,), stride=(2,))\n",
      ")\n",
      "In layer 5 Outputs are similar? FALSE\n",
      "MAE is: 9.10748099158809e-09\n",
      "MSE is: 1.630179112088303e-14\n",
      "\n",
      "ENCODER LAYER 6\n",
      "Keras: <Sequential name=sequential_5, built=True>\n",
      "Torch: Sequential(\n",
      "  (0): ReflectionPad1d((1, 1))\n",
      "  (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
      ")\n",
      "In layer 6 Outputs are similar? FALSE\n",
      "MAE is: 1.6981475425836834e-07\n",
      "MSE is: 9.141880910048736e-14\n",
      "\n",
      "ENCODER LAYER FLATTEN\n",
      "Keras: Reshaping (flattening before fc1)\n",
      "Torch: Reshaping (flattening before fc1)\n",
      "In layer FLATTEN Outputs are similar? FALSE\n",
      "MAE is: 1.6981475425836834e-07\n",
      "MSE is: 9.141880910048736e-14\n",
      "\n",
      "ENCODER LAYER FC1\n",
      "Keras: <Dense name=fc1, built=True>\n",
      "Torch: Linear(in_features=8192, out_features=32, bias=True)\n",
      "In layer 7 Outputs are similar? FALSE\n",
      "MAE is: 2.493643478373997e-06\n",
      "MSE is: 2.8589323616823314e-11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "random_streamlines = utils.prepare_tensor_from_file(trk, anat).numpy()\n",
    "dummy_input_keras = tf.convert_to_tensor(random_streamlines, dtype=tf.float32)\n",
    "dummy_input_torch = torch.Tensor(random_streamlines.transpose(0, 2, 1))\n",
    "\n",
    "for i, (layer_keras, layer_torch) in enumerate(zip(encoder_keras_layers[:-1], encoder_torch_layers[:-1])):\n",
    "    print(f\"ENCODER LAYER {i+1}\")\n",
    "    print(f\"Keras: {layer_keras}\")\n",
    "    print(f\"Torch: {layer_torch}\")\n",
    "    # Run the input through the layers\n",
    "    layer_output_keras = dummy_input_keras\n",
    "    layer_output_torch = dummy_input_torch\n",
    "    for c in range(i+1):\n",
    "        if c == 5:  # conv6, do not ReLU\n",
    "            layer_output_keras = encoder_keras_layers[c](layer_output_keras)\n",
    "            layer_output_torch = encoder_torch_layers[c](layer_output_torch)\n",
    "        else:\n",
    "            layer_output_keras = tf.nn.relu(encoder_keras_layers[c](layer_output_keras))\n",
    "            layer_output_torch = F.relu(encoder_torch_layers[c](layer_output_torch))\n",
    "    # Check if the outputs are similar\n",
    "    layer_output_torch_reshaped = layer_output_torch.detach().numpy().transpose(0, 2, 1)\n",
    "    are_close = np.all(np.isclose(layer_output_keras, layer_output_torch_reshaped, atol=1e-6))\n",
    "    print(f\"In layer {i+1} Outputs are similar? {str(are_close).upper()}\")\n",
    "    print(f\"MAE is: {np.mean(np.abs(layer_output_keras - layer_output_torch_reshaped))}\")\n",
    "    print(f\"MSE is: {np.mean((layer_output_keras - layer_output_torch_reshaped) ** 2)}\\n\")\n",
    "\n",
    "\n",
    "# reshape before running into fc1. Match PyTorch's behavior by transposing the output first\n",
    "encoder_out_size_keras = (layer_output_keras.shape[1], layer_output_keras.shape[2])\n",
    "h7_keras = tf.transpose(layer_output_keras, perm=[0, 2, 1])\n",
    "h7_keras = tf.keras.layers.Flatten()(h7_keras)\n",
    "\n",
    "encoder_out_size_torch = (layer_output_torch.shape[1], layer_output_torch.shape[2])\n",
    "h7_torch = layer_output_torch.view(-1, encoder_out_size_torch[0] * encoder_out_size_torch[1]).detach().numpy()\n",
    "\n",
    "print(\"ENCODER LAYER FLATTEN\")\n",
    "are_close = np.all(np.isclose(h7_keras, h7_torch, atol=1e-6))\n",
    "print(f\"Keras: Reshaping (flattening before fc1)\")\n",
    "print(f\"Torch: Reshaping (flattening before fc1)\")\n",
    "print(f\"In layer FLATTEN Outputs are similar? {str(are_close).upper()}\")\n",
    "print(f\"MAE is: {np.mean(np.abs(h7_keras - h7_torch))}\")\n",
    "print(f\"MSE is: {np.mean((h7_keras - h7_torch) ** 2)}\\n\")\n",
    "\n",
    "# fc1\n",
    "print(\"ENCODER LAYER FC1\")\n",
    "fc1_keras = tf.nn.relu(encoder_keras_layers[-1](h7_keras))\n",
    "fc1_torch = F.relu(encoder_torch_layers[-1](torch.Tensor(h7_torch))).detach().numpy()\n",
    "are_close = np.all(np.isclose(h7_keras, h7_torch, atol=1e-6))\n",
    "print(f\"Keras: {encoder_keras_layers[-1]}\")\n",
    "print(f\"Torch: {encoder_torch_layers[-1]}\")\n",
    "print(f\"In layer {7} Outputs are similar? {str(are_close).upper()}\")\n",
    "print(f\"MAE is: {np.mean(np.abs(fc1_keras - fc1_torch))}\")\n",
    "print(f\"MSE is: {np.mean((fc1_keras - fc1_torch) ** 2)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTPUTS ARE (SIMILAR)? IN THE ENCODER. LET'S CHECK THE DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECODER LAYER FC2\n",
      "Keras: <Dense name=fc2, built=True>\n",
      "Torch: Linear(in_features=32, out_features=8192, bias=True)\n",
      "In layer FC2 Outputs are similar? FALSE\n",
      "MAE: 3.006216786616278e-07\n",
      "MSE is: 6.112635137639488e-12\n",
      "\n",
      "ENCODER LAYER 1\n",
      "Keras: <Sequential name=sequential_6, built=True>\n",
      "Torch: Sequential(\n",
      "  (0): ReflectionPad1d((1, 1))\n",
      "  (1): Conv1d(1024, 512, kernel_size=(3,), stride=(1,))\n",
      ")\n",
      "In layer 1 Outputs are similar? FALSE\n",
      "MAE is: 2.4263886189146433e-06\n",
      "MSE is: 1.6551117421048644e-10\n",
      "\n",
      "ENCODER LAYER 2\n",
      "Keras: <Sequential name=sequential_7, built=True>\n",
      "Torch: Sequential(\n",
      "  (0): ReflectionPad1d((1, 1))\n",
      "  (1): Conv1d(512, 256, kernel_size=(3,), stride=(1,))\n",
      ")\n",
      "In layer 2 Outputs are similar? FALSE\n",
      "MAE is: 3.7567951949313283e-06\n",
      "MSE is: 2.109021846052883e-10\n",
      "\n",
      "ENCODER LAYER 3\n",
      "Keras: <Sequential name=sequential_8, built=True>\n",
      "Torch: Sequential(\n",
      "  (0): ReflectionPad1d((1, 1))\n",
      "  (1): Conv1d(256, 128, kernel_size=(3,), stride=(1,))\n",
      ")\n",
      "In layer 3 Outputs are similar? FALSE\n",
      "MAE is: 3.276526513218414e-06\n",
      "MSE is: 1.9646000104511785e-10\n",
      "\n",
      "ENCODER LAYER 4\n",
      "Keras: <Sequential name=sequential_9, built=True>\n",
      "Torch: Sequential(\n",
      "  (0): ReflectionPad1d((1, 1))\n",
      "  (1): Conv1d(128, 64, kernel_size=(3,), stride=(1,))\n",
      ")\n",
      "In layer 4 Outputs are similar? FALSE\n",
      "MAE is: 2.4320984266523737e-06\n",
      "MSE is: 6.16216591642349e-11\n",
      "\n",
      "ENCODER LAYER 5\n",
      "Keras: <Sequential name=sequential_10, built=True>\n",
      "Torch: Sequential(\n",
      "  (0): ReflectionPad1d((1, 1))\n",
      "  (1): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
      ")\n",
      "In layer 5 Outputs are similar? FALSE\n",
      "MAE is: 1.3016572211199673e-06\n",
      "MSE is: 1.485015582725069e-11\n",
      "\n",
      "ENCODER LAYER 6\n",
      "Keras: <Sequential name=sequential_11, built=True>\n",
      "Torch: Sequential(\n",
      "  (0): ReflectionPad1d((1, 1))\n",
      "  (1): Conv1d(32, 3, kernel_size=(3,), stride=(1,))\n",
      ")\n",
      "In layer 6 Outputs are similar? TRUE\n",
      "MAE is: 3.006216786616278e-07\n",
      "MSE is: 4.530090495433181e-13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: CHECK THIS CODE AGAIN/ MAYBE WRITE SEQUENTIALLY AND NOT LOOP\n",
    "# Decoder (fc1 is the input to it)\n",
    "\n",
    "print(\"DECODER LAYER FC2\")\n",
    "fc2_keras = tf.nn.relu(decoder_keras_layers[-1](fc1_keras))\n",
    "fc2_torch = F.relu(decoder_torch_layers[-1](torch.Tensor(fc1_torch))).detach().numpy()\n",
    "are_close = np.all(np.isclose(fc2_keras, fc2_torch, atol=1e-6))\n",
    "print(f\"Keras: {decoder_keras_layers[-1]}\")\n",
    "print(f\"Torch: {decoder_torch_layers[-1]}\")\n",
    "print(f\"In layer FC2 Outputs are similar? {str(are_close).upper()}\")\n",
    "print(f\"MAE: {np.mean(np.abs(layer_output_keras - layer_output_torch_reshaped))}\")\n",
    "print(f\"MSE is: {np.mean((fc2_keras - fc2_torch) ** 2)}\\n\")\n",
    "\n",
    "# Reshape before running into the first conv layer\n",
    "# Mimic PyTorch behavior by reshaping the output first, then transpose it for our weights to work\n",
    "fc2_keras_t = tf.reshape(fc2_keras, (-1, encoder_out_size_keras[1], encoder_out_size_keras[0]))\n",
    "fc2_keras = tf.transpose(fc2_keras_t, perm=[0, 2, 1])\n",
    "fc2_torch = torch.Tensor(fc2_torch).view(-1,\n",
    "                                         encoder_out_size_torch[0],\n",
    "                                         encoder_out_size_torch[1])\n",
    "\n",
    "for i, (layer_keras, layer_torch) in enumerate(zip(decoder_keras_layers[:-1], decoder_torch_layers[:-1])):\n",
    "    print(f\"ENCODER LAYER {i+1}\")\n",
    "    print(f\"Keras: {layer_keras}\")\n",
    "    print(f\"Torch: {layer_torch}\")\n",
    "    # Run the input through the layers\n",
    "    layer_output_keras = fc2_keras\n",
    "    layer_output_torch = fc2_torch\n",
    "    for c in range(i+1):\n",
    "        layer_output_keras = tf.nn.relu(decoder_keras_layers[c](layer_output_keras))\n",
    "        layer_output_torch = F.relu(decoder_torch_layers[c](layer_output_torch))\n",
    "    # Check if the outputs are similar\n",
    "    layer_output_torch_reshaped = np.transpose(layer_output_torch.detach().numpy(), (0, 2, 1))\n",
    "    are_close = np.all(np.isclose(layer_output_keras, layer_output_torch_reshaped, atol=1e-6))\n",
    "    print(f\"In layer {i+1} Outputs are similar? {str(are_close).upper()}\")\n",
    "    print(f\"MAE is: {np.mean(np.abs(layer_output_keras - layer_output_torch_reshaped))}\")\n",
    "    print(f\"MSE is: {np.mean((layer_output_keras - layer_output_torch_reshaped) ** 2)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "====================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put some tractograms into the Models, check the output similarity visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/fibercup/fibercup_advanced_filtering_no_ushapes/ae_input_std_endpoints/\"\n",
    "\n",
    "f = \"fibercup_Simulated_prob_tracking_minL10_resampled256_plausibles_std_endpoints_\"\n",
    "anat = os.path.abspath(\"../data/fibercup/Simulated_FiberCup.nii.gz\")\n",
    "tractogram_paths = [os.path.abspath(os.path.join(data_dir, \"train\", f + \"train.trk\")),\n",
    "                    os.path.abspath(os.path.join(data_dir, \"test\", f + \"test.trk\")),\n",
    "                    os.path.abspath(os.path.join(data_dir, \"valid\", f + \"valid.trk\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tractoencoder_gsoc import utils\n",
    "\n",
    "data_wd = os.path.abspath(\"./data_outputs/\")\n",
    "os.makedirs(data_wd, exist_ok=True)\n",
    "\n",
    "for trk_path in tractogram_paths:\n",
    "    # Load the tractogram\n",
    "    input_data_keras = utils.prepare_tensor_from_file(trk_path, anat)\n",
    "    output_keras = model_keras(input_data_keras)\n",
    "    \n",
    "    input_data_torch = torch.Tensor(input_data_keras.numpy().transpose(0, 2, 1))\n",
    "    output_torch = model_torch(input_data_torch)\n",
    "    \n",
    "    output_path_torch = os.path.join(data_wd, os.path.basename(trk_path).replace(\".trk\", \"_torch_output.trk\"))\n",
    "    output_path_keras = os.path.join(data_wd, os.path.basename(trk_path).replace(\".trk\", \"_keras_output.trk\"))\n",
    "    utils.save_tractogram(streamlines=output_keras,\n",
    "                          img_fname=anat,\n",
    "                          tractogram_fname=output_path_keras)\n",
    "    utils.save_tractogram(streamlines=output_keras,\n",
    "                          img_fname=anat,\n",
    "                          tractogram_fname=output_path_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT: Encode a tractogram with the Keras model, decode it with both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wd = os.path.abspath(\"./data_outputs/encode_keras_decode_both\")\n",
    "os.makedirs(data_wd, exist_ok=True)\n",
    "\n",
    "for trk in tractogram_paths[0:1]:\n",
    "    \n",
    "    # Take a tractogram from FiberCup\n",
    "    input_data_keras = utils.prepare_tensor_from_file(trk,\n",
    "                                                      anat)\n",
    "\n",
    "    # Run the input through the Keras encoder. Get the latent space representation of the input\n",
    "    output_encoder_keras = model_keras.model.get_layer('encoder')(input_data_keras)\n",
    "    # Also run the input through the PyTorch encoder. Compare them just in case\n",
    "    output_encoder_torch = model_torch.encode(torch.Tensor(input_data_keras.numpy().transpose(0, 2, 1)))\n",
    "    output_encoder_torch = output_encoder_torch.detach().numpy()\n",
    "    # Compare output_encoder_keras and output_encoder_torch\n",
    "    print(\"Encoder MSE: \", np.mean((output_encoder_keras - output_encoder_torch) ** 2))\n",
    "    print(\"Encoder MAE: \", np.mean(np.abs(output_encoder_keras - output_encoder_torch)))\n",
    "    print(\"Encoder std: \", np.std(output_encoder_keras - output_encoder_torch), \"\\n\")\n",
    "    # Decode the output of the encoder using the Keras model\n",
    "    output_model_keras = model_keras.model.get_layer('decoder')(output_encoder_keras)\n",
    "\n",
    "    # Decode the output of the encoder using the PyTorch model\n",
    "    output_model_torch = model_torch.decode(torch.Tensor(output_encoder_keras.numpy()))\n",
    "    output_model_torch = output_model_torch.detach().numpy().transpose(0, 2, 1)\n",
    "\n",
    "    # Check the difference between the outputs\n",
    "    print(\"MSE: \", np.mean((output_model_keras - output_model_torch) ** 2))\n",
    "    print(\"MAE: \", np.mean(np.abs(output_model_keras - output_model_torch)))\n",
    "    print(\"std: \", np.std(output_model_keras - output_model_torch), \"\\n\")\n",
    "    \n",
    "    output_path_torch = os.path.join(data_wd, os.path.basename(trk).replace(\".trk\", \"_torch_output.trk\"))\n",
    "    output_path_keras = os.path.join(data_wd, os.path.basename(trk).replace(\".trk\", \"_keras_output.trk\"))\n",
    "\n",
    "    # Save the output of the Keras model\n",
    "    utils.save_tractogram(streamlines=output_model_keras.numpy().transpose(0, 2, 1),\n",
    "                          img_fname=anat,\n",
    "                          tractogram_fname=output_path_keras)\n",
    "\n",
    "    # Save the output of the Torch model\n",
    "    utils.save_tractogram(streamlines=output_model_torch,\n",
    "                          img_fname=anat,\n",
    "                          tractogram_fname=output_path_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very strange thing happening here.\n",
    "\n",
    "### If we TORCH(ENCODE-DECODE) -> Bad reconstruction\n",
    "### If we KERAS(ENCODE-DECODE) -> Bad reconstruction (same as above)\n",
    "### If we KERAS(ENCODE) and TORCH(DECODE) -> Good reconstruction\n",
    "\n",
    "# WTH\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
