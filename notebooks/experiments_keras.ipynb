{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "import tractoencoder_gsoc.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read some TRK data:\n",
    "fibercup_path = \"/home/teitxe/data/FiberCup/\"\n",
    "data_path = \"/home/teitxe/data/FiberCup/fibercup_advanced_filtering_no_ushapes/\"\n",
    "f_trk_data = op.join(data_path, \"ae_input_std_endpoints/train/fibercup_Simulated_prob_tracking_minL10_resampled256_plausibles_std_endpoints_train.trk\")\n",
    "f_img_data = op.join(fibercup_path, \"Simulated_FiberCup.nii.gz\")\n",
    "streamlines = utils.read_data(f_trk_data, f_img_data)\n",
    "print(f\"N of streamlines: {len(streamlines)}\")\n",
    "print(f\"Example of a streamline point: {streamlines[0][0]}\")\n",
    "print(f\"N of points in the first streamline: {len(streamlines[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset to fetch from it during the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a tensorflow dataset out of the streamlines\n",
    "dataset = tf.data.Dataset.from_tensor_slices(streamlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Loss and the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: Mean squared error\n",
    "loss_mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def loss(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return loss_mse(y_true=y, y_pred=y_)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adadelta(learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse_results = []\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 1\n",
    "dataset_train_batch = dataset.batch(batch_size)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_mse = tf.keras.metrics.MeanSquaredError()\n",
    "    \n",
    "    for x in dataset_train_batch:\n",
    "        # Optimize the model\n",
    "        loss_value, grads = grad(model, x, x)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        # Track progress\n",
    "        epoch_mse.update_state(x, model(x))\n",
    "    \n",
    "    # End epoch\n",
    "    train_mse_results.append(epoch_mse.result())\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Loss: {epoch_mse.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', loss='mse')\n",
    "# model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a training loop iteration manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_value, gradients = grad(model, input_streamline, input_streamline)\n",
    "print(f\"Step: {optimizer.iterations.numpy()}, Initial Loss: {loss_value.numpy()}\")\n",
    "print(f\"Step: {optimizer.iterations.numpy()},         Loss: {loss(model, input_streamline, input_streamline).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a leading underscore to avoid function parameters shadowing these\n",
    "# variables\n",
    "_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "_train_tractogram_fname = \"strml_train.trk\"\n",
    "_valid_tractogram_fname = \"strml_valid.trk\"\n",
    "_img_fname = \"t1.nii.gz\"\n",
    "_trained_weights_fname = \"already_available_model_weights.pt\"\n",
    "_training_weights_fname = \"training_model_weights.pt\"\n",
    "# The following values were found to give best results\n",
    "_lr = 6.68e-4\n",
    "_weight_decay = 0.13\n",
    "_epochs = 100\n",
    "# resample_data()   # resample your tractogram to 256 points if needed\n",
    "test_ae_model(\n",
    "    _train_tractogram_fname, _img_fname, _device\n",
    ")  # only does a forward pass, does not train the model\n",
    "test_ae_model_loader(_train_tractogram_fname, _img_fname, _device)  # computes loss\n",
    "_ = load_model_weights(_trained_weights_fname, _device, _lr, _weight_decay)  # load model weights\n",
    "train_ae_model(\n",
    "    _train_tractogram_fname, _valid_tractogram_fname, _img_fname, _device, _lr, _weight_decay, _epochs, _training_weights_fname\n",
    ")  # computes loss and trains the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
